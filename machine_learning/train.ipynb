{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c1e8cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "from state.board import Board, Color\n",
    "from machine_learning.model import ChessNet\n",
    "from machine_learning.utils import board_to_tensor, get_material_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b142bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CẤU HÌNH ---\n",
    "EPISODES = 250       # Số ván tự chơi (Self-play)\n",
    "MAX_MOVES = 500      # Giới hạn số nước đi để tránh loop vô tận\n",
    "EPSILON_START = 0.9  # Tỉ lệ đi ngẫu nhiên ban đầu (khám phá)\n",
    "EPSILON_END = 0.1    # Tỉ lệ đi ngẫu nhiên lúc sau (tối ưu)\n",
    "EPSILON_DECAY = 0.995\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a3bf20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_move_epsilon(model, board, epsilon):\n",
    "    \"\"\"Chọn nước đi: Epsilon% ngẫu nhiên, (1-Epsilon)% theo Model\"\"\"\n",
    "    next_states = board.generate_next_states()\n",
    "    if not next_states: return None\n",
    "\n",
    "    # Khám phá (Exploration): Đi bừa để học cái mới\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(next_states)\n",
    "\n",
    "    # Khai thác (Exploitation): Đi nước tốt nhất theo Model hiện tại\n",
    "    best_state = None\n",
    "    best_score = -float('inf') if board.turn == Color.WHITE else float('inf')\n",
    "    \n",
    "    # Batch processing để nhanh hơn\n",
    "    tensor_list = []\n",
    "    for st in next_states:\n",
    "        tensor_list.append(board_to_tensor(st.board).numpy())\n",
    "    \n",
    "    batch_tensors = torch.tensor(np.array(tensor_list)).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        scores = model(batch_tensors).cpu().numpy().flatten()\n",
    "        \n",
    "    if board.turn == Color.WHITE:\n",
    "        idx = np.argmax(scores)\n",
    "    else:\n",
    "        idx = np.argmin(scores)\n",
    "        \n",
    "    return next_states[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d2127dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_self_play(episodes, max_moves, epsilon_start, epsilon_end, epsilon_decay, learning_rate, batch_size):\n",
    "    print(f\"--- Bắt đầu Training Self-Play trên {device} ---\")\n",
    "    \n",
    "    model = ChessNet().to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    epsilon = epsilon_start\n",
    "    memory = [] # Lưu trữ dữ liệu: (board_tensor, target_value)\n",
    "\n",
    "    for episode in range(1, episodes + 1):\n",
    "        board = Board()\n",
    "        game_history = [] # Lưu các trạng thái trong ván này\n",
    "        \n",
    "        # --- GIAI ĐOẠN 1: TỰ CHƠI (SELF-PLAY) ---\n",
    "        winner = 0 # 0: Hòa, 1: Trắng, -1: Đen\n",
    "        moves_count = 0\n",
    "        \n",
    "        while moves_count < max_moves:\n",
    "            # Kiểm tra kết thúc game\n",
    "            if not board._has_legal_moves_for(board.turn):\n",
    "                if board.is_in_check(board.turn):\n",
    "                    winner = -1 if board.turn == Color.WHITE else 1\n",
    "                else:\n",
    "                    winner = 0 # Hòa stalemate\n",
    "                break\n",
    "                \n",
    "            # Bot chọn nước đi\n",
    "            next_state = select_move_epsilon(model, board, epsilon)\n",
    "            if next_state is None: break # Hòa/Lỗi\n",
    "            \n",
    "            # Lưu trạng thái vào lịch sử ván đấu\n",
    "            # Note: Lưu tensor của bàn cờ TRƯỚC khi đi hay SAU khi đi?\n",
    "            # Với Value Network, ta đánh giá thế cờ HIỆN TẠI.\n",
    "            game_history.append(board_to_tensor(next_state.board))\n",
    "            \n",
    "            board = next_state.board\n",
    "            moves_count += 1\n",
    "            \n",
    "        # Giảm epsilon dần dần\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        # --- GIAI ĐOẠN 2: TẠO DỮ LIỆU TRAINING (REWARD ASSIGNMENT) ---\n",
    "        # Gán nhãn cho toàn bộ nước đi trong ván\n",
    "        # Nếu Trắng thắng (1): Mọi thế cờ dẫn đến kết quả này đều có xu hướng = 1\n",
    "        # Nhưng để thông minh hơn, ta cộng thêm điểm Material (heuristic) để dẫn hướng ban đầu\n",
    "        \n",
    "        for state_tensor in game_history:\n",
    "            # Reward shaping: Kết hợp Kết quả ván cờ + Lợi thế vật chất\n",
    "            # target = (Kết quả thực tế * 0.7) + (Điểm vật chất quy đổi * 0.3)\n",
    "            # Điều này giúp bot không bị \"mù\" khi chưa thắng ván nào\n",
    "            \n",
    "            # Tính điểm vật chất đơn giản (-1 đến 1)\n",
    "            # Giả sử max material diff là 20\n",
    "            # material_val = get_material_score(...) / 20.0 \n",
    "            # Tuy nhiên để đơn giản, ta dùng Pure Monte Carlo trước:\n",
    "            \n",
    "            target = float(winner) \n",
    "            \n",
    "            # Lưu vào bộ nhớ chung\n",
    "            memory.append((state_tensor, target))\n",
    "\n",
    "        # --- GIAI ĐOẠN 3: HUẤN LUYỆN (TRAINING) ---\n",
    "        # Chỉ train khi đủ dữ liệu hoặc hết ván\n",
    "        if len(memory) > batch_size:\n",
    "            # Lấy ngẫu nhiên batch để học (Experience Replay)\n",
    "            batch = random.sample(memory, batch_size)\n",
    "            states_b, targets_b = zip(*batch)\n",
    "            \n",
    "            states_tensor = torch.stack(states_b).to(device)\n",
    "            targets_tensor = torch.tensor(targets_b, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(states_tensor)\n",
    "            loss = criterion(outputs, targets_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Giữ bộ nhớ không quá lớn (quên bớt cái cũ)\n",
    "            if len(memory) > 5000:\n",
    "                memory = memory[-5000:]\n",
    "                \n",
    "        # Log kết quả\n",
    "        if episode % 10 == 0:\n",
    "            result_str = \"Hòa\"\n",
    "            if winner == 1: result_str = \"Trắng Thắng\"\n",
    "            if winner == -1: result_str = \"Đen Thắng\"\n",
    "            print(f\"Ep {episode}: {result_str} (Moves: {moves_count}, Eps: {epsilon:.2f}, Loss: {loss.item():.4f})\")\n",
    "            \n",
    "        # Lưu model định kỳ\n",
    "        if episode % 50 == 0:\n",
    "            torch.save(model.state_dict(), \"chess_model.pth\")\n",
    "\n",
    "    print(\"--- Hoàn tất Training RL ---\")\n",
    "    torch.save(model.state_dict(), \"chess_model.pth\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7b245f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bắt đầu Training Self-Play trên cuda ---\n",
      "Ep 10: Hòa (Moves: 500, Eps: 0.86, Loss: 0.1853)\n",
      "Ep 20: Hòa (Moves: 500, Eps: 0.81, Loss: 0.1465)\n",
      "Ep 30: Hòa (Moves: 500, Eps: 0.77, Loss: 0.0939)\n",
      "Ep 40: Hòa (Moves: 500, Eps: 0.74, Loss: 0.0810)\n",
      "Ep 50: Đen Thắng (Moves: 66, Eps: 0.70, Loss: 0.1221)\n",
      "Ep 60: Trắng Thắng (Moves: 199, Eps: 0.67, Loss: 0.0772)\n",
      "Ep 70: Hòa (Moves: 500, Eps: 0.63, Loss: 0.0834)\n",
      "Ep 80: Hòa (Moves: 500, Eps: 0.60, Loss: 0.1248)\n",
      "Ep 90: Hòa (Moves: 500, Eps: 0.57, Loss: 0.0029)\n",
      "Ep 100: Hòa (Moves: 500, Eps: 0.55, Loss: 0.0020)\n",
      "Ep 110: Hòa (Moves: 500, Eps: 0.52, Loss: 0.0942)\n",
      "Ep 120: Hòa (Moves: 500, Eps: 0.49, Loss: 0.1643)\n",
      "Ep 130: Hòa (Moves: 500, Eps: 0.47, Loss: 0.2048)\n",
      "Ep 140: Trắng Thắng (Moves: 107, Eps: 0.45, Loss: 0.1346)\n",
      "Ep 150: Hòa (Moves: 500, Eps: 0.42, Loss: 0.0619)\n",
      "Ep 160: Hòa (Moves: 500, Eps: 0.40, Loss: 0.0323)\n",
      "Ep 170: Hòa (Moves: 500, Eps: 0.38, Loss: 0.0660)\n",
      "Ep 180: Hòa (Moves: 500, Eps: 0.37, Loss: 0.0318)\n",
      "Ep 190: Hòa (Moves: 132, Eps: 0.35, Loss: 0.0838)\n",
      "Ep 200: Hòa (Moves: 500, Eps: 0.33, Loss: 0.1004)\n",
      "Ep 210: Hòa (Moves: 500, Eps: 0.31, Loss: 0.0306)\n",
      "Ep 220: Hòa (Moves: 500, Eps: 0.30, Loss: 0.0232)\n",
      "Ep 230: Hòa (Moves: 500, Eps: 0.28, Loss: 0.0308)\n",
      "Ep 240: Đen Thắng (Moves: 148, Eps: 0.27, Loss: 0.1019)\n",
      "Ep 250: Hòa (Moves: 500, Eps: 0.26, Loss: 0.0499)\n",
      "--- Hoàn tất Training RL ---\n"
     ]
    }
   ],
   "source": [
    "train_self_play(EPISODES, MAX_MOVES, EPSILON_START, EPSILON_END, EPSILON_DECAY, LEARNING_RATE, BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
